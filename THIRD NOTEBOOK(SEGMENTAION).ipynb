{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a guide on how to operate the optimized nn-unet used in our paper (PAPER TITLE)\n",
    "for more information you can visit the original repo of the segmentation network \"https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Segmentation/nnUNet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-first step is to clone the repo of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/DeepLearningExamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will creat the folder in which we will put our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxEtIYdszR_b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/content/data/BraTS2021_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mounting our drive contatinig the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unzipping the zipped data into our folder\n",
    "\n",
    "The training dataset provided for the BraTS21 challenge consists of 1,251 brain mpMRI scans along with segmentation annotations of tumorous regions. The 3D volumes were skull-stripped and resampled to 1 mm isotropic resolution, with dimensions of (240, 240, 155) voxels. For each example, four modalities were given: Fluid Attenuated Inversion Recovery (FLAIR), native (T1), post-contrast T1-weighted (T1Gd), and T2-weighted (T2). See image below with each modality. Annotations consist of four classes: 1 for necrotic tumor core (NCR), 2 for peritumoral edematous tissue (ED), 4 for enhancing tumor (ET), and 0 for background (voxels that are not part of the tumor).\n",
    "\n",
    "To download the training and validation dataset, you need to have an account on https://www.synapse.org platform and be registered for BraTS21 challenge. We will assume that after downloading and unzipping, the dataset is organized as follows:\n",
    "\n",
    "```\n",
    "/data \n",
    " │\n",
    " ├───BraTS2021_train\n",
    " │      ├──BraTS2021_00000 \n",
    " │      │      └──BraTS2021_00000_flair.nii.gz\n",
    " │      │      └──BraTS2021_00000_t1.nii.gz\n",
    " │      │      └──BraTS2021_00000_t1ce.nii.gz\n",
    " │      │      └──BraTS2021_00000_t2.nii.gz\n",
    " │      │      └──BraTS2021_00000_seg.nii.gz\n",
    " │      ├──BraTS2021_00002\n",
    " │      │      └──BraTS2021_00002_flair.nii.gz\n",
    " │      ...    └──...\n",
    " │\n",
    " └────BraTS2021_val\n",
    "        ├──BraTS2021_00001 \n",
    "        │      └──BraTS2021_00001_flair.nii.gz\n",
    "        │      └──BraTS2021_00001_t1.nii.gz\n",
    "        │      └──BraTS2021_00001_t1ce.nii.gz\n",
    "        │      └──BraTS2021_00001_t2.nii.gz\n",
    "        ├──BraTS2021_00002\n",
    "        │      └──BraTS2021_00002_flair.nii.gz\n",
    "        ...    └──...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "szdAXQRi_B4E",
    "outputId": "75506a63-c875-481f-c46f-81028b09874e"
   },
   "outputs": [],
   "source": [
    "# data is extracted to the dirve\n",
    "!unzip \"/content/drive/MyDrive/BraTS Validation Brain.zip\" -d \"/content/data/BraTS2021_train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhxCXlNqYXWI",
    "outputId": "9fc13abf-db43-40dd-aebb-9dc8cb4271e2"
   },
   "outputs": [],
   "source": [
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "JpJxzAZQYgCb",
    "outputId": "f85f02da-8ab8-4238-95cc-f19048f8defa"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning==1.9.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6QJEiP3jZI9",
    "outputId": "317b8210-6e8c-4bc1-ad40-45bfc583ee4a"
   },
   "outputs": [],
   "source": [
    "!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0asChkR-kw1",
    "outputId": "9cbaf419-61b8-4860-ddaf-5faad3e5c2ea"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "PojX2EVUmRAV",
    "outputId": "214d82eb-d58f-40f8-d522-be4443349dab"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/apex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "234i6G7jnaFY",
    "outputId": "c0b776bd-259b-4d2d-cffb-8729db2fe8f0"
   },
   "outputs": [],
   "source": [
    "!apt-get install ninja-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUVNJherkrpe",
    "outputId": "2c0795e3-54db-4b1d-ad08-07cc6e4dbe34"
   },
   "outputs": [],
   "source": [
    "!pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cuda_ext\" ./\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0zndsGVoxMo",
    "outputId": "5e80e65d-01a0-4605-935a-bfaa80cb3068"
   },
   "outputs": [],
   "source": [
    "!pip install  'git+https://github.com/NVIDIA/dllogger'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "biOSzVXcyNeJ",
    "outputId": "580634ea-07b3-4ab0-dda2-e65565e1449a"
   },
   "outputs": [],
   "source": [
    "!pip install -U rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "AnCJ9ph0jAIh"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each example of the BraTS21 dataset consists of four NIfTI files with different MRI modalities (filenames with suffixes flair, t1, t1ce, t2). Additionally, examples in the training dataset have a NIfTI with annotation (filename with suffix seg). As a first step of data pre-processing, all four modalities are stacked such that each example has shape (4, 240, 240, 155) (input tensor is in the (C, H, W, D) layout, where C-channels, H-height, W-width and D-depth). Then redundant background voxels (with voxel value zero) on the borders of each volume are cropped, as they do not provide any useful information and can be ignored by the neural network. Subsequently, for each example, the mean and the standard deviation are computed within the non-zero region for each channel separately. All volumes are normalized by first subtracting the mean and then divided by the standard deviation. The background voxels are not normalized so that their value remained at zero. To distinguish between background voxels and normalized voxels which have values close to zero, we add an input channel with one-hot encoding for foreground voxels and stacked with the input data. As a result, each example has 5 channels.\n",
    "\n",
    "Let's start by preparing the raw training and validation datasets into stacked NIfTI files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two versions of this code when for when flair is Gans made and one for when the flair is original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one below is for the Gans made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmXFMRQ5khtB",
    "outputId": "096dc3a9-a51d-49f9-d46b-7691d45c78d2"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from subprocess import call\n",
    "import time\n",
    "\n",
    "import nibabel\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "def load_nifty(directory, example_id, suffix):\n",
    "    return nibabel.load(os.path.join(directory, example_id + \"_\" + suffix + \".nii.gz\"))\n",
    "\n",
    "\n",
    "def load_channels(d, example_id):\n",
    "    return [load_nifty(d, example_id, suffix) for suffix in [\"flair\", \"t1\", \"t1ce\", \"t2\"]]\n",
    "\n",
    "\n",
    "def get_data(nifty, dtype=\"int16\"):\n",
    "    if dtype == \"int16\":\n",
    "        data = np.abs(nifty.get_fdata().astype(np.int16))\n",
    "        data[data == -32768] = 0\n",
    "        return data\n",
    "    return nifty.get_fdata().astype(np.uint8)\n",
    "\n",
    "def get_data_flair(nifty, dtype=\"int16\"):\n",
    "    if dtype == \"int16\":\n",
    "        data=nifty.get_fdata()\n",
    "        data=rescale_intensity(data, out_range=(0, 1))\n",
    "        data=data*32768\n",
    "        data = np.abs(data.astype(np.int16))\n",
    "        data[data == -32768] = 0\n",
    "        return data\n",
    "    return nifty.get_fdata().astype(np.uint8)\n",
    "\n",
    "def prepare_nifty(d):\n",
    "    example_id = d.split(\"/\")[-1]\n",
    "    flair, t1, t1ce, t2 = load_channels(d, example_id)\n",
    "    affine, header = t1.affine, t1.header\n",
    "    vol = np.stack([get_data_flair(flair), get_data(t1), get_data(t1ce), get_data(t2)], axis=-1)\n",
    "    vol = nibabel.nifti1.Nifti1Image(vol, affine, header=header)\n",
    "    nibabel.save(vol, os.path.join(d, example_id + \".nii.gz\"))\n",
    "\n",
    "    if os.path.exists(os.path.join(d, example_id + \"_seg.nii.gz\")):\n",
    "        seg = load_nifty(d, example_id, \"seg\")\n",
    "        affine, header = seg.affine, seg.header\n",
    "        vol = get_data(seg, \"unit8\")\n",
    "        vol[vol == 4] = 3\n",
    "        seg = nibabel.nifti1.Nifti1Image(vol, affine, header=header)\n",
    "        nibabel.save(seg, os.path.join(d, example_id + \"_seg.nii.gz\"))\n",
    "\n",
    "\n",
    "def prepare_dirs(data, train):\n",
    "    img_path, lbl_path = os.path.join(data, \"images\"), os.path.join(data, \"labels\")\n",
    "    call(f\"mkdir {img_path}\", shell=True)\n",
    "    if train:\n",
    "        call(f\"mkdir {lbl_path}\", shell=True)\n",
    "    dirs = glob(os.path.join(data, \"BraTS*\"))\n",
    "    for d in dirs:\n",
    "        if \"_\" in d.split(\"/\")[-1]:\n",
    "            files = glob(os.path.join(d, \"*.nii.gz\"))\n",
    "            for f in files:\n",
    "                if \"flair\" in f or \"t1\" in f or \"t1ce\" in f or \"t2\" in f:\n",
    "                    continue\n",
    "                if \"_seg\" in f:\n",
    "                    call(f\"mv {f} {lbl_path}\", shell=True)\n",
    "                else:\n",
    "                    call(f\"mv {f} {img_path}\", shell=True)\n",
    "        call(f\"rm -rf {d}\", shell=True)\n",
    "\n",
    "\n",
    "def prepare_dataset_json(data, train):\n",
    "    images, labels = glob(os.path.join(data, \"images\", \"*\")), glob(os.path.join(data, \"labels\", \"*\"))\n",
    "    images = sorted([img.replace(data + \"/\", \"\") for img in images])\n",
    "    labels = sorted([lbl.replace(data + \"/\", \"\") for lbl in labels])\n",
    "\n",
    "    modality = {\"0\": \"FLAIR\", \"1\": \"T1\", \"2\": \"T1CE\", \"3\": \"T2\"}\n",
    "    labels_dict = {\"0\": \"background\", \"1\": \"edema\", \"2\": \"non-enhancing tumor\", \"3\": \"enhancing tumour\"}\n",
    "    if train:\n",
    "        key = \"training\"\n",
    "        data_pairs = [{\"image\": img, \"label\": lbl} for (img, lbl) in zip(images, labels)]\n",
    "    else:\n",
    "        key = \"test\"\n",
    "        data_pairs = [{\"image\": img} for img in images]\n",
    "\n",
    "    dataset = {\n",
    "        \"labels\": labels_dict,\n",
    "        \"modality\": modality,\n",
    "        key: data_pairs,\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(data, \"dataset.json\"), \"w\") as outfile:\n",
    "        json.dump(dataset, outfile)\n",
    "\n",
    "\n",
    "def run_parallel(func, args):\n",
    "    return Parallel(n_jobs=os.cpu_count())(delayed(func)(arg) for arg in args)\n",
    "\n",
    "\n",
    "def prepare_dataset(data, train):\n",
    "    print(f\"Preparing BraTS21 dataset from: {data}\")\n",
    "    start = time.time()\n",
    "    run_parallel(prepare_nifty, sorted(glob(os.path.join(data, \"BraTS*\"))))\n",
    "    prepare_dirs(data, train)\n",
    "    prepare_dataset_json(data, train)\n",
    "    end = time.time()\n",
    "    print(f\"Preparing time: {(end - start):.2f}\")\n",
    "\n",
    "prepare_dataset(\"/content/data/BraTS2021_train\", True)\n",
    "#prepare_dataset(\"/data/BraTS2021_val\", False)\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can notice in the above code there is a commented line, you can use this line when you want to preprocess the data to only predict not to train as it assumes that there is no ground truth labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is for the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4EDB4GtCzIr_",
    "outputId": "27724830-8cda-4461-d2ca-cc5cb8461867"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from subprocess import call\n",
    "import time\n",
    "\n",
    "import nibabel\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "def load_nifty(directory, example_id, suffix):\n",
    "    return nibabel.load(os.path.join(directory, example_id + \"_\" + suffix + \".nii.gz\"))\n",
    "\n",
    "\n",
    "def load_channels(d, example_id):\n",
    "    return [load_nifty(d, example_id, suffix) for suffix in [\"flair\", \"t1\", \"t1ce\", \"t2\"]]\n",
    "\n",
    "\n",
    "def get_data(nifty, dtype=\"int16\"):\n",
    "    if dtype == \"int16\":\n",
    "        data = np.abs(nifty.get_fdata().astype(np.int16))\n",
    "        data[data == -32768] = 0\n",
    "        return data\n",
    "    return nifty.get_fdata().astype(np.uint8)\n",
    "\n",
    "\n",
    "def prepare_nifty(d):\n",
    "    example_id = d.split(\"/\")[-1]\n",
    "    flair, t1, t1ce, t2 = load_channels(d, example_id)\n",
    "    affine, header = t1.affine, t1.header\n",
    "    vol = np.stack([get_data(flair), get_data(t1), get_data(t1ce), get_data(t2)], axis=-1)\n",
    "    vol = nibabel.nifti1.Nifti1Image(vol, affine, header=header)\n",
    "    nibabel.save(vol, os.path.join(d, example_id + \".nii.gz\"))\n",
    "\n",
    "    if os.path.exists(os.path.join(d, example_id + \"_seg.nii.gz\")):\n",
    "        seg = load_nifty(d, example_id, \"seg\")\n",
    "        affine, header = seg.affine, seg.header\n",
    "        vol = get_data(seg, \"unit8\")\n",
    "        vol[vol == 4] = 3\n",
    "        seg = nibabel.nifti1.Nifti1Image(vol, affine, header=header)\n",
    "        nibabel.save(seg, os.path.join(d, example_id + \"_seg.nii.gz\"))\n",
    "\n",
    "\n",
    "def prepare_dirs(data, train):\n",
    "    img_path, lbl_path = os.path.join(data, \"images\"), os.path.join(data, \"labels\")\n",
    "    call(f\"mkdir {img_path}\", shell=True)\n",
    "    if train:\n",
    "        call(f\"mkdir {lbl_path}\", shell=True)\n",
    "    dirs = glob(os.path.join(data, \"BraTS*\"))\n",
    "    for d in dirs:\n",
    "        if \"_\" in d.split(\"/\")[-1]:\n",
    "            files = glob(os.path.join(d, \"*.nii.gz\"))\n",
    "            for f in files:\n",
    "                if \"flair\" in f or \"t1\" in f or \"t1ce\" in f or \"t2\" in f:\n",
    "                    continue\n",
    "                if \"_seg\" in f:\n",
    "                    call(f\"mv {f} {lbl_path}\", shell=True)\n",
    "                else:\n",
    "                    call(f\"mv {f} {img_path}\", shell=True)\n",
    "        call(f\"rm -rf {d}\", shell=True)\n",
    "\n",
    "\n",
    "def prepare_dataset_json(data, train):\n",
    "    images, labels = glob(os.path.join(data, \"images\", \"*\")), glob(os.path.join(data, \"labels\", \"*\"))\n",
    "    images = sorted([img.replace(data + \"/\", \"\") for img in images])\n",
    "    labels = sorted([lbl.replace(data + \"/\", \"\") for lbl in labels])\n",
    "\n",
    "    modality = {\"0\": \"FLAIR\", \"1\": \"T1\", \"2\": \"T1CE\", \"3\": \"T2\"}\n",
    "    labels_dict = {\"0\": \"background\", \"1\": \"edema\", \"2\": \"non-enhancing tumor\", \"3\": \"enhancing tumour\"}\n",
    "    if train:\n",
    "        key = \"training\"\n",
    "        data_pairs = [{\"image\": img, \"label\": lbl} for (img, lbl) in zip(images, labels)]\n",
    "    else:\n",
    "        key = \"test\"\n",
    "        data_pairs = [{\"image\": img} for img in images]\n",
    "\n",
    "    dataset = {\n",
    "        \"labels\": labels_dict,\n",
    "        \"modality\": modality,\n",
    "        key: data_pairs,\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(data, \"dataset.json\"), \"w\") as outfile:\n",
    "        json.dump(dataset, outfile)\n",
    "\n",
    "\n",
    "def run_parallel(func, args):\n",
    "    return Parallel(n_jobs=os.cpu_count())(delayed(func)(arg) for arg in args)\n",
    "\n",
    "\n",
    "def prepare_dataset(data, train):\n",
    "    print(f\"Preparing BraTS21 dataset from: {data}\")\n",
    "    start = time.time()\n",
    "    run_parallel(prepare_nifty, sorted(glob(os.path.join(data, \"BraTS*\"))))\n",
    "    prepare_dirs(data, train)\n",
    "    prepare_dataset_json(data, train)\n",
    "    end = time.time()\n",
    "    print(f\"Preparing time: {(end - start):.2f}\")\n",
    "\n",
    "prepare_dataset(\"/content/data/BraTS2021_train\", True)\n",
    "#prepare_dataset(\"/data/BraTS2021_val\", False)\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets preprocesses the datasets by cropping, adding the one hot encoding and normalizing the volumes. We will store the pre-processed volumes as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jy5anDgggWoO",
    "outputId": "d59151df-5d48-4e4c-927c-4d1b8e331864"
   },
   "outputs": [],
   "source": [
    "!python3 /content/DeepLearningExamples/PyTorch/Segmentation/nnUNet/preprocess.py --task 11  --exec_mode training --data \"/content/data\" --results \"/content/data\" --ohe\n",
    "#!python3 ../preprocess.py --task 12 --ohe --exec_mode test\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line commented above is used when predicting not training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below command is used for training the model , you can know more about each argument in the nividia repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvtvzlpfXJOV",
    "outputId": "ae0cb9cb-d09c-48ec-98d6-8ca322abfa26"
   },
   "outputs": [],
   "source": [
    "!python ../main.py --deep_supervision --depth 6 --filters 64 96 128 192 256 384 512 --min_fmap 2 --scheduler --learning_rate 0.0003 --epochs 30 --fold 0 --amp --gpus 1 --task 11 --save_ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below command is used for testing the model , you can know more about each argument in the nividia repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../main.py --exec_mode \"predict\" --gpus 1 --depth 6 --filters 64 96 128 192 256 384 512 --min_fmap 2 --amp --save_preds --task 11 --data \"/content/data/11_3d\" --ckpt_path \"path do model\" --results \"path do save inference\" --tta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will convert the .npy predections to the final predection form by taking the maximum propability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3WnNIgi1Hddg",
    "outputId": "941ee8ef-8771-4396-d4bc-859724fbc592"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from subprocess import call\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "\n",
    "def to_lbl(pred):\n",
    "    print(pred.shape)\n",
    "    pred = np.argmax(pred, axis=0)\n",
    "\n",
    "    \"\"\"\n",
    "    enh = pred[2]\n",
    "    c1, c2, c3 = pred[0] > 0.5, pred[1] > 0.5, pred[2] > 0.5\n",
    "    pred = (c1 > 0).astype(np.uint8)\n",
    "    pred[(c2 == False) * (c1 == True)] = 2\n",
    "    pred[(c3 == True) * (c1 == True)] = 4\n",
    "    print(pred.shape)\n",
    "    '''\n",
    "    components, n = label(pred == 4)\n",
    "    for et_idx in range(1, n + 1):\n",
    "        _, counts = np.unique(pred[components == et_idx], return_counts=True)\n",
    "        if 1 < counts[0] and counts[0] < 4 and np.mean(enh[components == et_idx]) < 0.9:\n",
    "            pred[components == et_idx] = 1\n",
    "\n",
    "    et = pred == 4\n",
    "    if 0 < et.sum() and et.sum() < 5 and np.mean(enh[et]) < 0.9:\n",
    "        pred[et] = 1\n",
    "    '''\n",
    "    \"\"\"\n",
    "\n",
    "    pred = np.transpose(pred, (2, 1, 0)).astype(np.uint8)\n",
    "    print(pred.shape)\n",
    "    return pred\n",
    "\n",
    "def prepare_preditions(e):\n",
    "    fname = e[0].split(\"/\")[-1].split(\".\")[0]\n",
    "    preds = [np.load(f) for f in e]\n",
    "    p = to_lbl(np.mean(preds, 0))\n",
    "    # the hirarchy should be like this\n",
    "    img = nib.load(f\"/content/drive/MyDrive/data/BraTS2021_train/images/{fname}.nii.gz\")\n",
    "    nib.save(\n",
    "        nib.Nifti1Image(p, img.affine, header=img.header),\n",
    "        os.path.join(\"/content/drive/MyDrive/final_preds\", fname + \".nii.gz\"),\n",
    "    )\n",
    "!rm -r \"/content/drive/MyDrive/final_preds\"\n",
    "os.makedirs(\"/content/drive/MyDrive/final_preds\")\n",
    "preds = sorted(glob(f\"/content/drive/MyDrive/predictions_epoch=7-dice=84_40_task=11_fold=0_tta*\"))\n",
    "examples = list(zip(*[sorted(glob(f\"{p}/*.npy\")) for p in preds]))\n",
    "print(\"Preparing final predictions\")\n",
    "for e in examples:\n",
    "    prepare_preditions(e)\n",
    "print(\"Finished!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
