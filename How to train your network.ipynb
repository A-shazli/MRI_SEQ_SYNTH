{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a guide on how to operate the optimized nn-unet used in our paper (PAPER TITLE)\n",
    "for more information you can visit the original repo of the segmentation network \"https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Segmentation/nnUNet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-first step is to clone the repo of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepLearningExamples'...\n",
      "remote: Enumerating objects: 33148, done.\u001b[K\n",
      "remote: Counting objects: 100% (971/971), done.\u001b[K\n",
      "remote: Compressing objects: 100% (667/667), done.\u001b[K\n",
      "remote: Total 33148 (delta 335), reused 858 (delta 279), pack-reused 32177\u001b[K\n",
      "Receiving objects: 100% (33148/33148), 106.84 MiB | 25.61 MiB/s, done.\n",
      "Resolving deltas: 100% (23249/23249), done.\n",
      "Updating files: 100% (5338/5338), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/DeepLearningExamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will creat the folder in which we will put our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxEtIYdszR_b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/content/data/BraTS2021_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mounting our drive contatinig the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unzipping the zipped data into our folder\n",
    "\n",
    "The training dataset provided for the BraTS21 challenge consists of 1,251 brain mpMRI scans along with segmentation annotations of tumorous regions. The 3D volumes were skull-stripped and resampled to 1 mm isotropic resolution, with dimensions of (240, 240, 155) voxels. For each example, four modalities were given: Fluid Attenuated Inversion Recovery (FLAIR), native (T1), post-contrast T1-weighted (T1Gd), and T2-weighted (T2). See image below with each modality. Annotations consist of four classes: 1 for necrotic tumor core (NCR), 2 for peritumoral edematous tissue (ED), 4 for enhancing tumor (ET), and 0 for background (voxels that are not part of the tumor).\n",
    "\n",
    "To download the training and validation dataset, you need to have an account on https://www.synapse.org platform and be registered for BraTS21 challenge. We will assume that after downloading and unzipping, the dataset is organized as follows:\n",
    "\n",
    "```\n",
    "/data \n",
    " │\n",
    " ├───BraTS2021_train\n",
    " │      ├──BraTS2021_00000 \n",
    " │      │      └──BraTS2021_00000_flair.nii.gz\n",
    " │      │      └──BraTS2021_00000_t1.nii.gz\n",
    " │      │      └──BraTS2021_00000_t1ce.nii.gz\n",
    " │      │      └──BraTS2021_00000_t2.nii.gz\n",
    " │      │      └──BraTS2021_00000_seg.nii.gz\n",
    " │      ├──BraTS2021_00002\n",
    " │      │      └──BraTS2021_00002_flair.nii.gz\n",
    " │      ...    └──...\n",
    " │\n",
    " └────BraTS2021_val\n",
    "        ├──BraTS2021_00001 \n",
    "        │      └──BraTS2021_00001_flair.nii.gz\n",
    "        │      └──BraTS2021_00001_t1.nii.gz\n",
    "        │      └──BraTS2021_00001_t1ce.nii.gz\n",
    "        │      └──BraTS2021_00001_t2.nii.gz\n",
    "        ├──BraTS2021_00002\n",
    "        │      └──BraTS2021_00002_flair.nii.gz\n",
    "        ...    └──...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "szdAXQRi_B4E",
    "outputId": "75506a63-c875-481f-c46f-81028b09874e"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/drive/MyDrive/BraTS Validation Brain.zip\" -d \"/content/data/BraTS2021_train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhxCXlNqYXWI",
    "outputId": "9fc13abf-db43-40dd-aebb-9dc8cb4271e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting monai\n",
      "  Downloading monai-1.2.0-202306081546-py3-none-any.whl (1.3 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai) (2.0.1+cu118)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from monai) (1.22.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (4.7.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->monai) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->monai) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->monai) (1.3.0)\n",
      "Installing collected packages: monai\n",
      "Successfully installed monai-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "JpJxzAZQYgCb",
    "outputId": "f85f02da-8ab8-4238-95cc-f19048f8defa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightning==1.9.4\n",
      "  Downloading pytorch_lightning-1.9.4-py3-none-any.whl (827 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m827.8/827.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (1.22.4)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (2.0.1+cu118)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (6.0)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (2023.6.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch_lightning==1.9.4)\n",
      "  Downloading torchmetrics-1.0.1-py3-none-any.whl (729 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m729.2/729.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.9.4) (4.7.1)\n",
      "Collecting lightning-utilities>=0.6.0.post0 (from pytorch_lightning==1.9.4)\n",
      "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (2.27.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (3.8.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (3.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->pytorch_lightning==1.9.4) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->pytorch_lightning==1.9.4) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->pytorch_lightning==1.9.4) (16.0.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->pytorch_lightning==1.9.4) (2.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning==1.9.4) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->pytorch_lightning==1.9.4) (1.3.0)\n",
      "Installing collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
      "Successfully installed lightning-utilities-0.9.0 pytorch_lightning-1.9.4 torchmetrics-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_lightning==1.9.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X6QJEiP3jZI9",
    "outputId": "317b8210-6e8c-4bc1-ad40-45bfc583ee4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://developer.download.nvidia.com/compute/redist\n",
      "Collecting nvidia-dali-cuda110\n",
      "  Downloading https://developer.download.nvidia.com/compute/redist/nvidia-dali-cuda110/nvidia_dali_cuda110-1.27.0-8625303-py3-none-manylinux2014_x86_64.whl (489.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.5/489.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda110) (1.6.3)\n",
      "Requirement already satisfied: gast>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda110) (0.4.0)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from nvidia-dali-cuda110) (0.1.8)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->nvidia-dali-cuda110) (0.40.0)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->nvidia-dali-cuda110) (1.16.0)\n",
      "Installing collected packages: nvidia-dali-cuda110\n",
      "Successfully installed nvidia-dali-cuda110-1.27.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --extra-index-url https://developer.download.nvidia.com/compute/redist --upgrade nvidia-dali-cuda110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0asChkR-kw1",
    "outputId": "9cbaf419-61b8-4860-ddaf-5faad3e5c2ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'apex'...\n",
      "remote: Enumerating objects: 11112, done.\u001b[K\n",
      "remote: Counting objects: 100% (238/238), done.\u001b[K\n",
      "remote: Compressing objects: 100% (130/130), done.\u001b[K\n",
      "remote: Total 11112 (delta 131), reused 185 (delta 107), pack-reused 10874\u001b[K\n",
      "Receiving objects: 100% (11112/11112), 15.39 MiB | 17.30 MiB/s, done.\n",
      "Resolving deltas: 100% (7677/7677), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "PojX2EVUmRAV",
    "outputId": "214d82eb-d58f-40f8-d522-be4443349dab"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/content/apex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "234i6G7jnaFY",
    "outputId": "c0b776bd-259b-4d2d-cffb-8729db2fe8f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  ninja-build\n",
      "0 upgraded, 1 newly installed, 0 to remove and 15 not upgraded.\n",
      "Need to get 107 kB of archives.\n",
      "After this operation, 338 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 ninja-build amd64 1.10.0-1build1 [107 kB]\n",
      "Fetched 107 kB in 1s (111 kB/s)\n",
      "Selecting previously unselected package ninja-build.\n",
      "(Reading database ... 123105 files and directories currently installed.)\n",
      "Preparing to unpack .../ninja-build_1.10.0-1build1_amd64.deb ...\n",
      "Unpacking ninja-build (1.10.0-1build1) ...\n",
      "Setting up ninja-build (1.10.0-1build1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get install ninja-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "UUVNJherkrpe",
    "outputId": "2c0795e3-54db-4b1d-ad08-07cc6e4dbe34"
   },
   "outputs": [],
   "source": [
    "!pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings \"--build-option=--cuda_ext\" ./\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "O0zndsGVoxMo",
    "outputId": "5e80e65d-01a0-4605-935a-bfaa80cb3068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/NVIDIA/dllogger\n",
      "  Cloning https://github.com/NVIDIA/dllogger to /tmp/pip-req-build-brhhbjgz\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/dllogger /tmp/pip-req-build-brhhbjgz\n",
      "  Resolved https://github.com/NVIDIA/dllogger to commit 0540a43971f4a8a16693a9de9de73c1072020769\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: DLLogger\n",
      "  Building wheel for DLLogger (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for DLLogger: filename=DLLogger-1.0.0-py3-none-any.whl size=5656 sha256=49a51dffa0959b604cc43d496d34b4a42076fc6bad62e2c2bb1be7fb5e8830e3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qk_e0eyb/wheels/41/52/02/4093763d9dcfd96d85b3d90dec4afdcf40b3934c0d41cd1f3c\n",
      "Successfully built DLLogger\n",
      "Installing collected packages: DLLogger\n",
      "Successfully installed DLLogger-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install  'git+https://github.com/NVIDIA/dllogger'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "biOSzVXcyNeJ",
    "outputId": "580634ea-07b3-4ab0-dda2-e65565e1449a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (13.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich) (2.14.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "AnCJ9ph0jAIh"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each example of the BraTS21 dataset consists of four NIfTI files with different MRI modalities (filenames with suffixes flair, t1, t1ce, t2). Additionally, examples in the training dataset have a NIfTI with annotation (filename with suffix seg). As a first step of data pre-processing, all four modalities are stacked such that each example has shape (4, 240, 240, 155) (input tensor is in the (C, H, W, D) layout, where C-channels, H-height, W-width and D-depth). Then redundant background voxels (with voxel value zero) on the borders of each volume are cropped, as they do not provide any useful information and can be ignored by the neural network. Subsequently, for each example, the mean and the standard deviation are computed within the non-zero region for each channel separately. All volumes are normalized by first subtracting the mean and then divided by the standard deviation. The background voxels are not normalized so that their value remained at zero. To distinguish between background voxels and normalized voxels which have values close to zero, we add an input channel with one-hot encoding for foreground voxels and stacked with the input data. As a result, each example has 5 channels.\n",
    "\n",
    "Let's start by preparing the raw training and validation datasets into stacked NIfTI files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two versions of this code when for when flair is Gans made and one for when the flair is original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one below is for the Gans made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmXFMRQ5khtB",
    "outputId": "096dc3a9-a51d-49f9-d46b-7691d45c78d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing BraTS21 dataset from: /content/data/BraTS2021_train\n",
      "Preparing time: 425.32\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from subprocess import call\n",
    "import time\n",
    "\n",
    "import nibabel\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "def load_nifty(directory, example_id, suffix):\n",
    "    return nibabel.load(os.path.join(directory, example_id + \"_\" + suffix + \".nii.gz\"))\n",
    "\n",
    "\n",
    "def load_channels(d, example_id):\n",
    "    return [load_nifty(d, example_id, suffix) for suffix in [\"flair\", \"t1\", \"t1ce\", \"t2\"]]\n",
    "\n",
    "\n",
    "def get_data(nifty, dtype=\"int16\"):\n",
    "    if dtype == \"int16\":\n",
    "        data = np.abs(nifty.get_fdata().astype(np.int16))\n",
    "        data[data == -32768] = 0\n",
    "        return data\n",
    "    return nifty.get_fdata().astype(np.uint8)\n",
    "\n",
    "def get_data_flair(nifty, dtype=\"int16\"):\n",
    "    if dtype == \"int16\":\n",
    "        data=nifty.get_fdata()\n",
    "        data=rescale_intensity(data, out_range=(0, 1))\n",
    "        data=data*32768\n",
    "        data = np.abs(data.astype(np.int16))\n",
    "        data[data == -32768] = 0\n",
    "        return data\n",
    "    return nifty.get_fdata().astype(np.uint8)\n",
    "\n",
    "def prepare_nifty(d):\n",
    "    example_id = d.split(\"/\")[-1]\n",
    "    flair, t1, t1ce, t2 = load_channels(d, example_id)\n",
    "    affine, header = t1.affine, t1.header\n",
    "    vol = np.stack([get_data_flair(flair), get_data(t1), get_data(t1ce), get_data(t2)], axis=-1)\n",
    "    vol = nibabel.nifti1.Nifti1Image(vol, affine, header=header)\n",
    "    nibabel.save(vol, os.path.join(d, example_id + \".nii.gz\"))\n",
    "\n",
    "    if os.path.exists(os.path.join(d, example_id + \"_seg.nii.gz\")):\n",
    "        seg = load_nifty(d, example_id, \"seg\")\n",
    "        affine, header = seg.affine, seg.header\n",
    "        vol = get_data(seg, \"unit8\")\n",
    "        vol[vol == 4] = 3\n",
    "        seg = nibabel.nifti1.Nifti1Image(vol, affine, header=header)\n",
    "        nibabel.save(seg, os.path.join(d, example_id + \"_seg.nii.gz\"))\n",
    "\n",
    "\n",
    "def prepare_dirs(data, train):\n",
    "    img_path, lbl_path = os.path.join(data, \"images\"), os.path.join(data, \"labels\")\n",
    "    call(f\"mkdir {img_path}\", shell=True)\n",
    "    if train:\n",
    "        call(f\"mkdir {lbl_path}\", shell=True)\n",
    "    dirs = glob(os.path.join(data, \"BraTS*\"))\n",
    "    for d in dirs:\n",
    "        if \"_\" in d.split(\"/\")[-1]:\n",
    "            files = glob(os.path.join(d, \"*.nii.gz\"))\n",
    "            for f in files:\n",
    "                if \"flair\" in f or \"t1\" in f or \"t1ce\" in f or \"t2\" in f:\n",
    "                    continue\n",
    "                if \"_seg\" in f:\n",
    "                    call(f\"mv {f} {lbl_path}\", shell=True)\n",
    "                else:\n",
    "                    call(f\"mv {f} {img_path}\", shell=True)\n",
    "        call(f\"rm -rf {d}\", shell=True)\n",
    "\n",
    "\n",
    "def prepare_dataset_json(data, train):\n",
    "    images, labels = glob(os.path.join(data, \"images\", \"*\")), glob(os.path.join(data, \"labels\", \"*\"))\n",
    "    images = sorted([img.replace(data + \"/\", \"\") for img in images])\n",
    "    labels = sorted([lbl.replace(data + \"/\", \"\") for lbl in labels])\n",
    "\n",
    "    modality = {\"0\": \"FLAIR\", \"1\": \"T1\", \"2\": \"T1CE\", \"3\": \"T2\"}\n",
    "    labels_dict = {\"0\": \"background\", \"1\": \"edema\", \"2\": \"non-enhancing tumor\", \"3\": \"enhancing tumour\"}\n",
    "    if train:\n",
    "        key = \"training\"\n",
    "        data_pairs = [{\"image\": img, \"label\": lbl} for (img, lbl) in zip(images, labels)]\n",
    "    else:\n",
    "        key = \"test\"\n",
    "        data_pairs = [{\"image\": img} for img in images]\n",
    "\n",
    "    dataset = {\n",
    "        \"labels\": labels_dict,\n",
    "        \"modality\": modality,\n",
    "        key: data_pairs,\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(data, \"dataset.json\"), \"w\") as outfile:\n",
    "        json.dump(dataset, outfile)\n",
    "\n",
    "\n",
    "def run_parallel(func, args):\n",
    "    return Parallel(n_jobs=os.cpu_count())(delayed(func)(arg) for arg in args)\n",
    "\n",
    "\n",
    "def prepare_dataset(data, train):\n",
    "    print(f\"Preparing BraTS21 dataset from: {data}\")\n",
    "    start = time.time()\n",
    "    run_parallel(prepare_nifty, sorted(glob(os.path.join(data, \"BraTS*\"))))\n",
    "    prepare_dirs(data, train)\n",
    "    prepare_dataset_json(data, train)\n",
    "    end = time.time()\n",
    "    print(f\"Preparing time: {(end - start):.2f}\")\n",
    "\n",
    "prepare_dataset(\"/content/data/BraTS2021_train\", True)\n",
    "#prepare_dataset(\"/data/BraTS2021_val\", False)\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can notice in the above code there is a commented line, you can use this line when you want to preprocess the data to only predict not to train as it assumes that there is no ground truth labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is for the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4EDB4GtCzIr_",
    "outputId": "27724830-8cda-4461-d2ca-cc5cb8461867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing BraTS21 dataset from: /content/data/BraTS2021_train\n",
      "Preparing time: 382.80\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from subprocess import call\n",
    "import time\n",
    "\n",
    "import nibabel\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "def load_nifty(directory, example_id, suffix):\n",
    "    return nibabel.load(os.path.join(directory, example_id + \"_\" + suffix + \".nii.gz\"))\n",
    "\n",
    "\n",
    "def load_channels(d, example_id):\n",
    "    return [load_nifty(d, example_id, suffix) for suffix in [\"flair\", \"t1\", \"t1ce\", \"t2\"]]\n",
    "\n",
    "\n",
    "def get_data(nifty, dtype=\"int16\"):\n",
    "    if dtype == \"int16\":\n",
    "        data = np.abs(nifty.get_fdata().astype(np.int16))\n",
    "        data[data == -32768] = 0\n",
    "        return data\n",
    "    return nifty.get_fdata().astype(np.uint8)\n",
    "\n",
    "\n",
    "def prepare_nifty(d):\n",
    "    example_id = d.split(\"/\")[-1]\n",
    "    flair, t1, t1ce, t2 = load_channels(d, example_id)\n",
    "    affine, header = t1.affine, t1.header\n",
    "    vol = np.stack([get_data(flair), get_data(t1), get_data(t1ce), get_data(t2)], axis=-1)\n",
    "    vol = nibabel.nifti1.Nifti1Image(vol, affine, header=header)\n",
    "    nibabel.save(vol, os.path.join(d, example_id + \".nii.gz\"))\n",
    "\n",
    "    if os.path.exists(os.path.join(d, example_id + \"_seg.nii.gz\")):\n",
    "        seg = load_nifty(d, example_id, \"seg\")\n",
    "        affine, header = seg.affine, seg.header\n",
    "        vol = get_data(seg, \"unit8\")\n",
    "        vol[vol == 4] = 3\n",
    "        seg = nibabel.nifti1.Nifti1Image(vol, affine, header=header)\n",
    "        nibabel.save(seg, os.path.join(d, example_id + \"_seg.nii.gz\"))\n",
    "\n",
    "\n",
    "def prepare_dirs(data, train):\n",
    "    img_path, lbl_path = os.path.join(data, \"images\"), os.path.join(data, \"labels\")\n",
    "    call(f\"mkdir {img_path}\", shell=True)\n",
    "    if train:\n",
    "        call(f\"mkdir {lbl_path}\", shell=True)\n",
    "    dirs = glob(os.path.join(data, \"BraTS*\"))\n",
    "    for d in dirs:\n",
    "        if \"_\" in d.split(\"/\")[-1]:\n",
    "            files = glob(os.path.join(d, \"*.nii.gz\"))\n",
    "            for f in files:\n",
    "                if \"flair\" in f or \"t1\" in f or \"t1ce\" in f or \"t2\" in f:\n",
    "                    continue\n",
    "                if \"_seg\" in f:\n",
    "                    call(f\"mv {f} {lbl_path}\", shell=True)\n",
    "                else:\n",
    "                    call(f\"mv {f} {img_path}\", shell=True)\n",
    "        call(f\"rm -rf {d}\", shell=True)\n",
    "\n",
    "\n",
    "def prepare_dataset_json(data, train):\n",
    "    images, labels = glob(os.path.join(data, \"images\", \"*\")), glob(os.path.join(data, \"labels\", \"*\"))\n",
    "    images = sorted([img.replace(data + \"/\", \"\") for img in images])\n",
    "    labels = sorted([lbl.replace(data + \"/\", \"\") for lbl in labels])\n",
    "\n",
    "    modality = {\"0\": \"FLAIR\", \"1\": \"T1\", \"2\": \"T1CE\", \"3\": \"T2\"}\n",
    "    labels_dict = {\"0\": \"background\", \"1\": \"edema\", \"2\": \"non-enhancing tumor\", \"3\": \"enhancing tumour\"}\n",
    "    if train:\n",
    "        key = \"training\"\n",
    "        data_pairs = [{\"image\": img, \"label\": lbl} for (img, lbl) in zip(images, labels)]\n",
    "    else:\n",
    "        key = \"test\"\n",
    "        data_pairs = [{\"image\": img} for img in images]\n",
    "\n",
    "    dataset = {\n",
    "        \"labels\": labels_dict,\n",
    "        \"modality\": modality,\n",
    "        key: data_pairs,\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(data, \"dataset.json\"), \"w\") as outfile:\n",
    "        json.dump(dataset, outfile)\n",
    "\n",
    "\n",
    "def run_parallel(func, args):\n",
    "    return Parallel(n_jobs=os.cpu_count())(delayed(func)(arg) for arg in args)\n",
    "\n",
    "\n",
    "def prepare_dataset(data, train):\n",
    "    print(f\"Preparing BraTS21 dataset from: {data}\")\n",
    "    start = time.time()\n",
    "    run_parallel(prepare_nifty, sorted(glob(os.path.join(data, \"BraTS*\"))))\n",
    "    prepare_dirs(data, train)\n",
    "    prepare_dataset_json(data, train)\n",
    "    end = time.time()\n",
    "    print(f\"Preparing time: {(end - start):.2f}\")\n",
    "\n",
    "prepare_dataset(\"/content/data/BraTS2021_train\", True)\n",
    "#prepare_dataset(\"/data/BraTS2021_val\", False)\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets preprocesses the datasets by cropping, adding the one hot encoding and normalizing the volumes. We will store the pre-processed volumes as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jy5anDgggWoO",
    "outputId": "d59151df-5d48-4e4c-927c-4d1b8e331864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-13 16:23:58.668951: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-13 16:24:00.114150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Preprocessing /content/data/BraTS2021_train\n",
      "2023-07-13 16:24:16.784492: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-13 16:24:16.802472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Pre-processing time: 637.68\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "!python3 /content/DeepLearningExamples/PyTorch/Segmentation/nnUNet/preprocess.py --task 11  --exec_mode training --data \"/content/data\" --results \"/content/data\" --ohe\n",
    "#!python3 ../preprocess.py --task 12 --ohe --exec_mode test\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line commented above is used when predicting not training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below command is used for training the model , you can know more about each argument in the nividia repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvtvzlpfXJOV",
    "outputId": "ae0cb9cb-d09c-48ec-98d6-8ca322abfa26"
   },
   "outputs": [],
   "source": [
    "!python ../main.py --deep_supervision --depth 6 --filters 64 96 128 192 256 384 512 --min_fmap 2 --scheduler --learning_rate 0.0003 --epochs 30 --fold 0 --amp --gpus 1 --task 11 --save_ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the below command is used for testing the model , you can know more about each argument in the nividia repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../main.py --exec_mode \"predict\" --gpus 1 --depth 6 --filters 64 96 128 192 256 384 512 --min_fmap 2 --amp --save_preds --task 11 --data \"/content/data/11_3d\" --ckpt_path \"/content/drive/MyDrive/epoch=7-dice=84.40.ckpt\" --results \"/content/drive/MyDrive/africa_60_nividia_results\" --tta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will convert the .npy predections to the final predection form by taking the maximum propability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3WnNIgi1Hddg",
    "outputId": "941ee8ef-8771-4396-d4bc-859724fbc592"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from subprocess import call\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "\n",
    "def to_lbl(pred):\n",
    "    print(pred.shape)\n",
    "    pred = np.argmax(pred, axis=0)\n",
    "\n",
    "    \"\"\"\n",
    "    enh = pred[2]\n",
    "    c1, c2, c3 = pred[0] > 0.5, pred[1] > 0.5, pred[2] > 0.5\n",
    "    pred = (c1 > 0).astype(np.uint8)\n",
    "    pred[(c2 == False) * (c1 == True)] = 2\n",
    "    pred[(c3 == True) * (c1 == True)] = 4\n",
    "    print(pred.shape)\n",
    "    '''\n",
    "    components, n = label(pred == 4)\n",
    "    for et_idx in range(1, n + 1):\n",
    "        _, counts = np.unique(pred[components == et_idx], return_counts=True)\n",
    "        if 1 < counts[0] and counts[0] < 4 and np.mean(enh[components == et_idx]) < 0.9:\n",
    "            pred[components == et_idx] = 1\n",
    "\n",
    "    et = pred == 4\n",
    "    if 0 < et.sum() and et.sum() < 5 and np.mean(enh[et]) < 0.9:\n",
    "        pred[et] = 1\n",
    "    '''\n",
    "    \"\"\"\n",
    "\n",
    "    pred = np.transpose(pred, (2, 1, 0)).astype(np.uint8)\n",
    "    print(pred.shape)\n",
    "    return pred\n",
    "\n",
    "def prepare_preditions(e):\n",
    "    fname = e[0].split(\"/\")[-1].split(\".\")[0]\n",
    "    preds = [np.load(f) for f in e]\n",
    "    p = to_lbl(np.mean(preds, 0))\n",
    "\n",
    "    img = nib.load(f\"/content/drive/MyDrive/finally_black_flair/data/BraTS2021_train/images/{fname}.nii.gz\")\n",
    "    nib.save(\n",
    "        nib.Nifti1Image(p, img.affine, header=img.header),\n",
    "        os.path.join(\"/content/drive/MyDrive/finally_black_flair/final_preds\", fname + \".nii.gz\"),\n",
    "    )\n",
    "!rm -r \"/content/drive/MyDrive/finally_black_flair/final_preds\"\n",
    "os.makedirs(\"/content/drive/MyDrive/finally_black_flair/final_preds\")\n",
    "preds = sorted(glob(f\"/content/drive/MyDrive/finally_black_flair/predictions_epoch=7-dice=84_40_task=11_fold=0_tta*\"))\n",
    "examples = list(zip(*[sorted(glob(f\"{p}/*.npy\")) for p in preds]))\n",
    "print(\"Preparing final predictions\")\n",
    "for e in examples:\n",
    "    prepare_preditions(e)\n",
    "print(\"Finished!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
